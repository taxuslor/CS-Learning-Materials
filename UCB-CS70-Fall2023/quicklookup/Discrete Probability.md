- Random experimen: In general, a random experiment consists of drawing a sample of *k* elements from a set *S* of cardinality *n*. The outcome of a random experiment is called a *sample point*, and the *sample space* (often denoted by Î©) is the set of all possible outcomes of the experiment.

- A probability space is a sample space Î©, together with a probability P[Ï‰] (often also denoted as Pr[Ï‰]) for each sample point Ï‰, such that

  - (Non-negativity): 0 â‰¤ P[Ï‰] â‰¤ 1 for all Ï‰ âˆˆ Î©.
  - (Total one): âˆ‘(Ï‰âˆˆÎ©) P[Ï‰] = 1.

- Definition 14.1 (Conditional Probability). *For events A, B âŠ† Î© in the same probability space such that* P[*B*] > 0*, the conditional probability of A given B is* P[*A*|*B*]= P[Aâˆ©B]/P[B]

- Bayesian Inference: A way to update knowledge after making an observation.

  - For example, we may have an estimate of the probability of a given event *A*. After event *B* occurs, we can update this estimate to P[*A* | *B*]. In this interpretation, P[*A*] can be thought of as a *prior* probability: our assessment of the likelihood of an event of interest, *A*, *before* making an observation. It reflects our prior knowledge. P[*A* | *B*] can be interpreted as the *posterior* probability of *A* after the observation. It reflects our updated knowledge.

- Bayesâ€™ Rule: P[*A*|*B*]= P[Aâˆ©B]/P[B] = P[B|A]\*P[A]

- Total Probability Rule: P[B]=P[Aâˆ©B]+P[Â¬Aâˆ©B]=P[B|A]P[A] + P[B|Â¬A]P[Â¬A]=P[B|A]P[A] + P[B|Â¬A]\*(1-P[A])

- Definition 14.2 (Partition of an event). *We say that an event A is partitioned into n events A*1,...,*An* if

  1. A* = *A*1 âˆª*A*2 âˆªÂ·Â·Â·âˆª*A**n**, and*

  2. Ai âˆ© *Aj* = âˆ… *for all i* â‰  *j (i.e., A*1,...,*An* *are mutually exclusive).*

- Definition 14.3 (Independence). *Two events A*,*B in the same probability space are said to be independent if* P[*A*âˆ©*B*] = P[*A*]Ã—P[*B*]*.* P[A|B] = P[A]

- Definition 14.4 (Mutual independence). *Events A*1,...,*A**n* *are said to be mutually independent if for* *every* *subset I* âŠ† {1,...,*n*} *with size* |*I*| â‰¥ 2*,* P[âˆ©*i*âˆˆ*I**A**i*] = âˆP[*Ai*].

- Definition 14.5 (Mutual independence). *Events A*1,...,*A**n* *are said to be mutually independent if for all* *Bi* âˆˆ {*Ai*,Â¬*Ai*},*i* = 1,...,*n,* P[*B*1 âˆ©Â·Â·Â·âˆ©*Bn*] = âˆ(i=1,n)P[*Bi*].

- Theorem 14.1 (Product Rule). *For any events A*, *B, we have* P[*A*âˆ©*B*]=P[*A*]P[*B*|*A*].

- Theorem 14.2 (Inclusion-Exclusion). *Let A*1 , . . . , *A**n* *be events in some probability space, where n* â‰¥ 2*. Then, we have* P[*A*1 âˆªÂ·Â·Â·âˆª*An*] = âˆ‘(k=1,n)(âˆ’1)^(*k*âˆ’1) âˆ‘(SâŠ†{1,...,n}:|S| = k)P[âˆ©(*i*âˆˆ*S*)*Ai*]

- Definition 15.1 (Random Variable). *A random variable X on a sample space* Î© *is a function X* : Î© â†’ R *that assigns to each sample point* Ï‰ âˆˆ Î© *a real number X*(Ï‰)*.*

- Definition 15.2 (Distribution). *The distribution of a discrete random variable X is the collection of values* {(*a*,P[*X* = *a*]) : *a* âˆˆ A }*, where* A *is the set of all possible values taken by X.*

- Bernoulli Distribution: *X* âˆ¼ Bernoulli(*p*)  P[*X* =*i*]=
  									p, if i =1
  									1-p, if i =0

- Binomial Distribution: *X* âˆ¼ Bin(*n*, *p*)  P[*X*=*i*]= (n,i)p^i(1âˆ’*p*)^(n-i) , for*i*=0,1,...,*n*.

- Hypergeometric Distribution: *Y* âˆ¼ Hypergeometric(*N*,*B*,*n*)  P[Y=k] = (n,k)(N-B,n-k)/(N,n)

- Definition 15.3. *The joint distribution for two discrete random variables X and Y is the collection of values* {((*a*,*b*),P[*X*=*a*,*Y*=*b*]):*a*âˆˆA,*b*âˆˆB},where *A* is the set of all possible values taken by X and *B* is the set of all possible values taken by Y. 

- Marginal distribution: given a joint distribution for *X* and *Y* , the distribution P[*X* = *a*] for *X* is called the *marginal distribution* for *X*,  and can be found by â€œsummingâ€ over the values of *Y*.  P[*X* =*a*]= âˆ‘(bâˆˆB)P[*X* =*a*,*Y* =*b*].

- Definition 15.4 (Independence). *Random variables X and Y on the same probability space are said to be* independent *if the events X* = *a and Y* = *b are independent for all values a*, *b. Equivalently, the joint distribution of independent r.v.â€™s decomposes as* P[*X* =*a*,*Y* =*b*]=P[*X* =*a*]P[*Y* =*b*], âˆ€*a*,*b*.

- Definition 15.5 (Expectation). *The expectation of a discrete random variable X is defined as E[X*]= âˆ‘(*a*âˆˆA)*a*Ã—P[*X*=*a*],  *where the sum is over all possible values taken by the r.v.*

- Theorem 15.1. *For any two random variables X and Y on the same probability space, we have* E[*X* +*Y*] = E[*X*]+E[*Y*]. *Also, for any constant c, we have* E[*cX*] = *c*E[*X*].

- Definition 16.1 (Variance). *For a r.v. X with expectation* E[*X*] = Î¼*, the* variance *of X is defined to be* Var(*X*) = E(*X* âˆ’ Î¼)2. *The square root* Ïƒ(*X*) := âˆš(Var(*X*)) *is called the* standard deviation *of X.*

- Theorem16.1. Forar.v.X with expectation *E[*X*] = Î¼*, we have *Var(*X*) = E*[X<sup>2</sup>]âˆ’Î¼<sup>2</sup>*.*

- Theorem 16.2. *For* independent *random variables X*,*Y, we have* E[*XY*] = E[*X*]E[*Y*]*.*

- Theorem 16.3. *For* independent *random variables X*,*Y, we have* Var(*X* +*Y*) = Var(*X*)+Var(*Y*).

- Definition 16.2 (Covariance). *The* covariance *of random variables X and Y , denoted* Cov(*X* , *Y* )*, is defined as* Cov(*X* , *Y* ) = E[(*X* âˆ’ Î¼*X* )(*Y* âˆ’ Î¼*Y* )] = E[*X Y* ] âˆ’ E[*X* ] E[*Y* ], *where* Î¼*X* = E[*X*] *and* Î¼*Y* = E[*Y*]*.*

- Definition 16.3 (Correlation). *Suppose X and Y are random variables with* Ïƒ (*X* ) > 0 *and* Ïƒ (*Y* ) > 0*. Then, the correlation of X and Y is defined as* Corr(*X*,*Y*)= Cov(*X*,*Y*)/Ïƒ(*X*)Ïƒ(*Y*)

- Theorem 16.4. *For any pair of random variables X and Y with* Ïƒ(*X*) > 0 *and* Ïƒ(*Y*) > 0*,* âˆ’1 â‰¤ Corr(*X*,*Y*) â‰¤ +1.

- Theorem 17.1 (Markovâ€™s Inequality). *For a* nonnegative *random variable X (i.e., X*(Ï‰) â‰¥ 0 *for all* Ï‰ âˆˆ Î©*) with finite mean,* P[Xâ‰¥c] â‰¤ E[X]/c *for any positive constant c.*

- Theorem 17.2 (Generalized Markovâ€™s Inequality). *Let Y be an arbitrary random variable with finite mean. Then, for any positive constants c and r,* P[|*Y*|â‰¥*c*] â‰¤ E[|*Y*|<sup>*r*</sup>] /*c* <sup>*r*</sup>

- Theorem 17.3 (Chebyshevâ€™s Inequality). *For a random variable X with finite expectation* E[*X*] = Î¼*,* P[|*X*âˆ’Î¼|â‰¥*c*]â‰¤ Var(*X*)/c<sup>2</sup>, *and for any positive constant c.*

- Theorem17.4(Law of Large Numbers). *Let X*1, *X*2,...,*be a sequence of i. i.d.(independent and identically distributed) random variables with common finite expectation* E[*Xi*] = Î¼ *for all i. Then, their partial sums Sn* = *X*1 +*X*2 +Â·Â·Â·+*Xn* *satisfy* P[/*n**S**n*âˆ’Î¼ <Îµ] â†’1 *as n*â†’ âˆ, *for every* Îµ > 0*, however small.*

- Definition 19.1 (Geometric Distribution). *A random variable X for which* P[*X* =*i*]=(1âˆ’*p*)<sup>*i*âˆ’1</sup>*p*, *for i*=1,2,3,..., *is said to have the geometric distribution with parameter p. This is abbreviated as X* âˆ¼ Geometric(*p*)*.*

- Theorem 19.1 (Tail Sum Formula). *Let X be a random variable that takes values in* {0,1,2,...}*. Then* E[*X*] = âˆ‘(i=1,âˆ) P[*X* â‰¥ *i*].

- Theorem 19.2. *For X* âˆ¼ Geometric( *p*)*, we have* E[*X* ] = 1 / *p* *.*

- Theorem 19.3. *For X* âˆ¼ Geometric( *p*)*, we have* Var (*X*) = 1âˆ’ *p* / p<sup>2</sup>*.*

- Definition 19.2 (Poisson distribution). *A random variable X for which* P[*X*=*i*]= Î»*i* / *i*! \* *e*<sup>âˆ’Î»</sup> , *for i*=0,1,2,...  *is said to have the Poisson distribution with parameter* Î» *. This is abbreviated as X* âˆ¼ Poisson(Î» )*.*

- Theorem 19.4. *For a Poisson random variable X* âˆ¼ Poisson(Î» )*, we have* E[*X* ] = Î» *and* Var (*X* ) = Î» *.*

- Theorem 19.5. *Let X* âˆ¼ Poisson(Î» ) *and Y* âˆ¼ Poisson(Î¼ ) *be independent Poisson random variables. Then, X* +*Y* âˆ¼ Poisson(Î» + Î¼)*.*

- Theorem 19.6. *Let X* âˆ¼ Binomial(*n*, Î»/*n* ) *where* Î» > 0 *is a fixed constant. Then for every i* = 0,1,2,...*,* P[*X* = *i*] â†’ Î»*i* / *i*! \* *e*<sup>âˆ’Î»</sup> as n â†’ âˆ.

- Definition 20.1. *The joint distribution for two discrete random variables X and Y is the collection of values* {((*a*,*b*),P[*X*=*a*,*Y*=*b*]):*a*âˆˆA,*b*âˆˆB}*,where* A is the set of all possible values taken by X and *B* is the set of all possible values taken by Y .

- *The law of iterated expectations:* E[*X*]=E[E[*X* |*Y*]]= âˆ‘(yâˆˆB)E[*X* |*Y* =*y*]P[*Y* =*y*].

- Definition 21.1 (Probability Density Function). *A* probability density function *(p.d.f.) for a real-valued random variable X is a function f* : R â†’ R *satisfying:*

  1. f is non-negative: f(*x*) â‰¥ 0 *for all x* âˆˆ R*.*
  2. The total integral of f is equal to 1: âˆ«(-âˆ,âˆ)f(x)dx = 1.
     *Then the distribution of X is given by:*

  P[*a*â‰¤*X* â‰¤*b*]=âˆ«(a,b)f(x)dx for all a < b.

- Definition 21.2 (Expectation). *The expectation of a continuous r.v. X with probability density function f is* E[*X*]= âˆ«(-âˆ,âˆ) *xf*(*x*)*dx*. 

- Definition 21.3 (Variance). *The variance of a continuous r.v. X with probability density function f is* Var(*X*)=E [(*X*âˆ’E[*X*])<sup>2</sup>] =E [*X*<sup>2</sup>] âˆ’E[*X*]<sup>2</sup> =âˆ«(-âˆ,âˆ)*x<sup>2</sup> f*(*x*)*dx* âˆ’ (âˆ«(-âˆ,âˆ)*xf*(*x*)*dx*)<sup>2</sup> .

- Definition 21.4 (Joint Density). *A* joint density function *for two random variable X and Y is a function f* : R<sup>2</sup> â†’ R *satisfying:*

  1. f is non-negative: f*(*x*,y*) â‰¥ 0 *for all x*,*y* âˆˆ R.

  2. The total integral of f is equal to*1*:âˆ¬ *f*(*x*,*y*)*dxdy*=1*.*

  *Then the joint distribution of X and Y is given by:*
  P[*a*â‰¤*X* â‰¤*b*, *c*â‰¤*Y* â‰¤*d*]=âˆ¬f(x,y)dxdy for all *a*â‰¤b and *c*â‰¤d

- Theorem 21.1. *The joint density of independent r.v.â€™s X and Y is the product of the marginal densities:*

  *f*(*x*,*y*)= *f*<sub>*X*</sub>(*x*)*f*<sub>*Y*</sub>(*y*) *for all x*,*y*âˆˆR.

- Definition 21.7 (Normal Distribution). *For any* Î¼ âˆˆ R *and* Ïƒ > 0*, a continuous random variable X with p.d.f.* *f*(*x*) = 1/âˆš2Ï€Ïƒ<sup>2</sup> \* *e*<sup>âˆ’(*x*âˆ’Î¼)<sup>2</sup>/(2Ïƒ)<sup>2</sup></sup> *is called a normal random variable with parameters* Î¼ *and* Ïƒ<sup>2</sup>*, and we write X* âˆ¼ *N*(Î¼,Ïƒ<sup>2</sup>)*. In the special case* Î¼ = 0 *and* Ïƒ = 1*, X is said to have the standard normal distribution.*

- Theorem 21.5 (Central Limit Theorem). *Let X*1,*X*2,... *be a sequence of i.i.d. random variables with common finite expectation* E[*X* *i*] = Î¼ *and finite variance* Var(*X* *i*) = Ïƒ<sup>2</sup>*. Let S* *n* = âˆ‘(*n*,*i*=1) *X**i**. Then, the distribution* *of* *Sn*âˆ’*n*Î¼ / âˆšÏƒ*n* *converges to N*(0,1) *as n* â†’ âˆ*. In other words, for any constant c* âˆˆ R*,* P [*Sn*âˆ’*n*Î¼ / âˆšÏƒ*n*â‰¤*c*] â†’ 1 / âˆš2ğ… âˆ«*e<sup> -x<sup>2</sup>/2</sup>dx* as *n* â†’ âˆ.

  

  

  

  

  

  

  